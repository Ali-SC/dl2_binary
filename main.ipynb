{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%-*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar 15 10:11:54 2021\n",
    "\n",
    "@author: ali.kadhim\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "#%%  Generate Training Data\n",
    "train_labels = []\n",
    "train_samples = []\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    # The ~5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1)\n",
    "    \n",
    "    # The ~5% of older individuals who did not experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0)\n",
    "\n",
    "for i in range(1000):\n",
    "    # The ~95% of younger individuals who did not experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0)\n",
    "    \n",
    "    # The ~95% of older individuals who did experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1)\n",
    "    \n",
    "# for i in train_samples:\n",
    "#     print(i)\n",
    "#\n",
    "# for i in train_labels:\n",
    "#     print(i)\n",
    "    \n",
    "\n",
    "#%% shuffle order and normalize data\n",
    "\n",
    "# put labels and samples into an array so you can shuffle/randomize the order of data for training\n",
    "# shuffling/randomizing is important this early due to the shuffling when training being applied after\n",
    "# pulling the validation dataset (0.1)\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "train_labels, train_samples = shuffle(train_labels, train_samples)\n",
    "\n",
    "# make all sample ages between 13 and 100 to between 0 and 1; scaling for algorithm\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))\n",
    "\n",
    "# for i in scaled_train_samples:\n",
    "#     print(i)\n",
    "    \n",
    "#%% Simple tf.keras Sequential Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "\n",
    "\n",
    "\n",
    "#%% Set up model\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "# log_dir = os.getcwd() + \"\\\\logs\\\\fit\\\\\"\n",
    "# # log_dir = \"logs\\fit\"\n",
    "# os.makedirs(log_dir)\n",
    "\n",
    "# tensorboard_callback = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=1)\n",
    "\n",
    "# logdir=mylogs:C:\\path\\to\\output\\folder\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "                                                      \n",
    "model.fit(\n",
    "    x=scaled_train_samples,\n",
    "    y=train_labels,\n",
    "    validation_split=0.1,\n",
    "    batch_size=10,\n",
    "    epochs=30,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "assert model.history.history.get('accuracy')[-1] > 0.90\n",
    "assert model.history.history.get('val_accuracy')[-1] > 0.90\n",
    "\n",
    "\n",
    "\n",
    "#%% Generate Test Data\n",
    "\n",
    "test_labels = []\n",
    "test_samples = []\n",
    "\n",
    "for i in range(10):\n",
    "    # The 5% of younger individuals who did experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)\n",
    "    \n",
    "    # The 5% of older individuals who did not experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "\n",
    "for i in range(200):\n",
    "    # The 95% of younger individuals who did not experience side effects\n",
    "    random_younger = randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0)\n",
    "    \n",
    "    # The 95% of older individuals who did experience side effects\n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1)\n",
    "    \n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "test_labels, test_samples = shuffle(test_labels, test_samples)\n",
    "\n",
    "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1,1))\n",
    "\n",
    "#%% Predict\n",
    "\n",
    "predictions = model.predict(x=scaled_test_samples, batch_size=10, verbose=0) \n",
    "\n",
    "rounded_predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "#%% Confusion Matrix & Plotting\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_true=test_labels, y_pred=rounded_predictions)\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion Matrix',\n",
    "                          cmap=plt.cm.Blues\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting 'normalize=True'.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix without normalization\")\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max()/2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i,j] > thresh else \"black\")\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        \n",
    "cm_plot_labels = ['no_side_effects', 'had_side_effects']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title = 'Confusion Matrix')\n",
    "                 \n",
    "\n",
    "#%% Save and Load a Model\n",
    "\n",
    "import os.path\n",
    "if os.path.isfile('models/ali_trial_model.h5') is False:\n",
    "    model.save('models/ali_trial_model.h5')\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model('models/ali_trial_model.h5')\n",
    "\n",
    "\n",
    "#%% Model to JSON\n",
    "\n",
    "json_string = model.to_json()\n",
    "\n",
    "# with open('json_model.json', 'w') as outfile:\n",
    "#     json.dump(json_string[0], outfile)\n",
    "\n",
    "\n",
    "#%% JSON to Model    \n",
    "# model reconstruction from JSON:\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# with open('json_model.json','r') as json_file:\n",
    "#     json_string_new = json.load(json_file)\n",
    "    \n",
    "json_model = model_from_json(json_string)\n",
    "json_model.summary()\n",
    "\n",
    "\n",
    "#%% Model.save_weights()\n",
    "\n",
    "import os.path\n",
    "if os.path.isfile('models/ali_model_weights.h5') is False:\n",
    "    model.save_weights('models/ali_model_weights.h5')\n",
    "    \n",
    "model2 = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "    \n",
    "    \n",
    "model2.load_weights('models/ali_model_weights.h5')\n",
    "model2.get_weights()\n",
    "\n",
    "model2.get_config()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
